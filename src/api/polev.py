import re
import asyncio
from typing import List, Optional, Dict, Any

from bs4 import BeautifulSoup

from src.session.aiohttp import AiohttpSession
from src.utils.user_agent import get_user_agent
from src.core.settings import load_settings
from src.utils.logger import Logger


class PolevAPI:

    API: str = 'https://polev.md'

    def __init__(
            self, 
            tasks_starts_at_once: int = 50, 
            proxy: Optional[str] = None,
            logger: Optional[Logger] = None
    ) -> None:
        self._session = AiohttpSession(api=self.API, proxy=proxy)
        self._semaphore = asyncio.Semaphore(tasks_starts_at_once)
        self.settings = load_settings()
        self.logger = logger or Logger()
        self._headers = {
                'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
                'accept-language': 'ru-RU,ru;q=0.8',
        }
        self._cookies = {}

    async def __aenter__(self) -> "PolevAPI":
        return self
    
    async def __aexit__(self, *args) -> None:
        await self._session.close()

    async def _make_request(self, url: str, start: int = None) -> str:
        """Ð£Ð½Ð¸Ð²ÐµÑ€ÑÐ°Ð»ÑŒÐ½Ñ‹Ð¹ Ð¼ÐµÑ‚Ð¾Ð´ Ð´Ð»Ñ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°."""
        
        full_url = f"{url}?start={start}" if start else url
        self._headers['user-agent'] = get_user_agent() 

        response = await self._session(
                'GET', 
                full_url, 
                headers=self._headers, 
                cookies=self._cookies,
            )
        
        return response

    async def get_categories(self) -> Dict[str, Any]:

        html = await self._make_request(f'{self.API}/ru/tovary.html')
        soup = BeautifulSoup(html, "lxml")

        categories = {}
        modcontent_div = soup.find('aside', attrs={'class': 'col-md-3 col-sm-4'})
        categories_a_tags = modcontent_div.find_all('a')
        for a in categories_a_tags[:7]:  
            url = a.get('href')
            name = a.text.strip()
            if url and name:
                categories[name] = f'{self.API}{url}'
        
        return categories
        
    
    async def get_all_urls_in_category(self, url: str):
        
        html = await self._make_request(url)
        if not html:
            return None

        categories_data = {}
        soup = BeautifulSoup(html, "lxml")
        subcatalog = soup.find_all('a', attrs={'class': 'product_link'})
    
        if not subcatalog:
            if url == 'https://polev.md/ru/tovary/tools.html':
                self.logger.info(f'ðŸ“ŒÐ”Ð¾Ð±Ð°Ð²Ð¸Ð» ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸ÑŽ: Ð˜Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ñ‹')
                categories_data["Ð˜Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ñ‹"] = url  
            # print('Ñ‚ÑƒÑ‚ ÑƒÐ¶Ðµ ÐºÐ°Ñ‚Ð°Ð»Ð¾Ð³ Ñ‚Ð¾Ð²Ð°Ñ€Ð¾Ð²')
            pass
        else:
            for category in subcatalog:
                # div_name = category.find('div', attrs={'class': 'categories-page__title'})
                name = category.text.strip()
                url = category.get('href')
                categories_data[name] = f'{self.API}{url}'
                self.logger.info(f'ðŸ“ŒÐ”Ð¾Ð±Ð°Ð²Ð¸Ð» ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸ÑŽ: {name}')

            for n,u in list(categories_data.items()):
                result = await self.get_all_urls_in_category(u)
                if result: 
                    categories_data.pop(n)
                    categories_data.update(result) 

        return categories_data


    async def check_page_num(self, url: str) -> int:
        """ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÑ‚ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ† Ñ Ñ‚Ð¾Ð²Ð°Ñ€Ð°Ð¼Ð¸ Ñ‡ÐµÑ€ÐµÐ· Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€ start Ð² Ð¿Ð°Ð³Ð¸Ð½Ð°Ñ†Ð¸Ð¸.
        Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Ð¾Ð±Ñ‰ÐµÐµ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ† (Ð½Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ start Ð¿Ð¾ÑÐ»ÐµÐ´Ð½ÐµÐ¹ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹)."""
        
        html = await self._make_request(url)
        soup = BeautifulSoup(html, "lxml")

        table = soup.find('table', class_='jshop_pagination')
        if not table:
            return 1  # ÐµÑÐ»Ð¸ Ð½ÐµÑ‚ Ð¿Ð°Ð³Ð¸Ð½Ð°Ñ†Ð¸Ð¸ - Ñ‚Ð¾Ð»ÑŒÐºÐ¾ 1 ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ð°
        
        pages = table.find_all('a')
        if not pages:
            return 1
            
        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ ÑÑÑ‹Ð»ÐºÑƒ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½ÐµÐ¹ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹
        last_page_a = pages[-1]
        last_page_url = last_page_a.get('href')
        
        # Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€ start
        match = re.search(r'start=(\d+)', last_page_url)
        if not match:
            return 1
            
        start_value = int(match.group(1))
        page_number = (start_value // 12) + 1  # Ð¿Ñ€ÐµÐ´Ð¿Ð¾Ð»Ð°Ð³Ð°ÐµÐ¼, Ñ‡Ñ‚Ð¾ ÑˆÐ°Ð³ Ð¿Ð°Ð³Ð¸Ð½Ð°Ñ†Ð¸Ð¸ 12
        
        return page_number

    async def get_all_products(self, url: str) -> List[str]:
        """Ð¡Ð¾Ð±Ð¸Ñ€Ð°ÐµÑ‚ ÑÑÑ‹Ð»ÐºÐ¸ Ð½Ð° Ð²ÑÐµ Ñ‚Ð¾Ð²Ð°Ñ€Ñ‹ ÑÐ¾ Ð²ÑÐµÑ… ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ† ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ð¸."""
        
        total_pages = await self.check_page_num(url)

        product_links = []
        for page in range(total_pages):
            # Ð¤Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÐµÐ¼ URL Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð¹ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹
            page_url = f"{url}?start={page * 12}" if page > 0 else url
            
            try:
                html = await self._make_request(page_url)
                soup = BeautifulSoup(html, "lxml")
                products = soup.find_all('td', class_='block_product')
                
                for product in products:
                    div_name = product.find('div', class_='name')
                    a_tag = div_name.find('a')
                    product_url = a_tag.get('href')
                    if product_url:
                        product_links.append(f'{self.API}{product_url}')
                        
            except Exception as e:
                self.logger.error(f"Error processing page {page + 1}: {str(e)}")
                continue
                
        return product_links


    async def get_html_product(self, url: str):

        response = await self._make_request(url)

        return response
