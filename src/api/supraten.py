
import re
import math
import asyncio
from typing import List, Optional, Dict, Any

from bs4 import BeautifulSoup
from aiohttp import ClientConnectorError

from src.session.aiohttp import AiohttpSession
from src.utils.user_agent import get_user_agent
from src.core.settings import load_settings
from src.utils.logger import Logger


class SupratenAPI:

    API: str = 'https://supraten.md/'

    def __init__(
            self, 
            tasks_starts_at_once: int = 100, 
            proxy: Optional[str] = None,
            logger: Optional[Logger] = None
    ) -> None:
        self._session = AiohttpSession(api=self.API, proxy=proxy)
        self._semaphore = asyncio.Semaphore(tasks_starts_at_once)
        self.settings = load_settings()
        self.logger = logger or Logger()
        self._headers = {
            'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'accept-language': 'ru-RU,ru;q=0.8',
        }

    async def __aenter__(self) -> "SupratenAPI":
        return self
    
    async def __aexit__(self, *args) -> None:
        await self._session.close()

    async def get_categories(self) -> Dict[str, Any]:

        self._headers['user-agent'] = get_user_agent()
        response = await self._session(
            'GET', 
            self.API, 
            headers=self._headers, 
        )

        categories_data = {}
        soup = BeautifulSoup(response, "lxml")
        ul_element = soup.find('ul', attrs={'class': 'sp-header-menu-category__list'})
        categories = ul_element.find_all('a', attrs={'class': 'sp-header-menu-category__link'})
        
        for category in categories:
            name = category.text.strip()
            href = category.get('href').strip()
            categories_data[name] = href

        return categories_data
    
    async def get_all_urls_in_category(self, url: str):
        
        self._headers['user-agent'] = get_user_agent()

        try:
            response = await self._session(
                'GET', 
                url, 
                headers=self._headers, 
            )
        except ClientConnectorError as ex:
            await asyncio.sleep(5)
            response = await self._session(
                'GET', 
                url, 
                headers=self._headers, 
            )
        

        if not response:
            return None
        
        categories_data = {}
        soup = BeautifulSoup(response, "lxml")
        div_cat_list = soup.find('div', attrs={'class': 'row sp-category-list'})

        if div_cat_list is None:
            # print(f"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –∫–∞—Ç–µ–≥–æ—Ä–∏–π –Ω–∞ {url}")
            return {}

        sub_categories = div_cat_list.find_all(
            'a', attrs={'class': 'sp-category-list__title'}
        )

        for category in sub_categories:
            name = category.text.strip()
            href = category.get('href').strip()

            # print(f"üìå –ù–∞–π–¥–µ–Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏—è: {name} -> {href}")
            self.logger.info(f"üìå –ù–∞–π–¥–µ–Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏—è: {name} -> {href}")
            categories_data[name] = href

            # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –≤—ã–∑—ã–≤–∞–µ–º –¥–ª—è –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            sub_data = await self.get_all_urls_in_category(href)
            categories_data.update(sub_data)  # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ

        return categories_data
    
    
    async def get_all_products(self, url: str):
        self._headers['user-agent'] = get_user_agent()

        # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è HTML-–∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        async def fetch_page(page: int = 1):
            
            response = await self._session(
                'GET', 
                f'{url}?limit=90&page={page}&', 
                headers=self._headers, 
            )
            
            return response

        # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –æ –ø—Ä–æ–¥—É–∫—Ç–∞—Ö —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        def parse_products(soup):
            div_sp_products = soup.find('div', attrs={'class': 'sp-products'})
            if div_sp_products is None:
                return []

            links_products = div_sp_products.find_all(
                'div', attrs={'class': 'sp-show-product-vertical'}
            )
            return [product.find('a').get('href') for product in links_products]

        # –®–∞–≥ 1: –ü–æ–ª—É—á–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü
        first_page_response = await fetch_page(page=1)
        soup = BeautifulSoup(first_page_response, "lxml")

        total_products = soup.find('span', attrs={'class': 'c-second-gray fs-14'})
        if total_products:
            digits = re.search(r'\d+', total_products.text)
            if digits:
                total_products = int(digits.group())
                # print(f"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–¥—É–∫—Ç–æ–≤: {total_products}")
            else:
                self.logger.info(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–¥—É–∫—Ç–æ–≤.")
                return []
        else:
            # print(f"–≠–ª–µ–º–µ–Ω—Ç —Å –æ–±—â–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω. {url}")
            return []

        # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü
        total_pages = math.ceil(total_products / 90)
        # print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {total_pages}")

        # –®–∞–≥ 2: –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å–æ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
        data = []
        for page in range(1, total_pages + 1):
            # print(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page}...")
            page_response = await fetch_page(page)
            soup = BeautifulSoup(page_response, 'lxml')
            products = parse_products(soup)
            data.extend(products)

        self.logger.info(f"–°–æ–±—Ä–∞–Ω–æ {len(data)} –ø—Ä–æ–¥—É–∫—Ç–æ–≤ —Å {total_pages} —Å—Ç—Ä–∞–Ω–∏—Ü. | {url}")
        return data

    
    async def get_html_product(self, url: str):

        self._headers['user-agent'] = get_user_agent()

        response = await self._session(
            'GET', 
            f'{url}', 
            headers=self._headers, 
        )

        if not response:
            response = await self._session(
            'GET', 
            f'{url}', 
            headers=self._headers, 
        )

        return response


    
        # async def get_product_data(self, url: str):
        
    #     self._headers['user-agent'] = get_user_agent()

    #     response = await self._session(
    #         'GET', 
    #         f'{url}', 
    #         headers=self._headers, 
    #     )

    #     data = {}

    #     soup = BeautifulSoup(response, 'html.parser')
    #     name = soup.find('h1', attrs={'class': 'sp-single-product__title'})
    #     data['name'] = name.text.strip()

    #     articul = soup.find('div', attrs={'class': 'sp-single-product__sku'})
    #     articul = articul.text.split(':')[-1].strip()
    #     data['articul'] = articul

    #     category = soup.find_all('li', attrs={'class': 'sp-breadcrumbs__item'})
    #     data['category'] = category[-2].text.strip()

    #     img = soup.select('a[data-lightbox="product-slider"]')[0].get("href")
    #     data['img_url'] = img

    #     price = soup.find('p', attrs={'class': 'sp-single-product__price-current'})
    #     data['price'] = price.text.split('/')[0]
        
    #     characteristics = soup.select('#characteristic')[-1]
    #     table = characteristics.find('table', class_='table table-bordered')
    #     rows = table.find('tbody').find_all('tr')

    #     for row in rows:
    #         key = row.find_all('td')[0].get_text(strip=True)  # –ü–µ—Ä–≤—ã–π <td> ‚Äî –∫–ª—é—á
    #         value = row.find_all('td')[1].get_text(strip=True)  # –í—Ç–æ—Ä–æ–π <td> ‚Äî –∑–Ω–∞—á–µ–Ω–∏–µ
    #         data[key] = value  

    #     return data
