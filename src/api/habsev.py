
import re
import math
import asyncio
import lxml.html
from typing import List, Optional, Dict, Any

from bs4 import BeautifulSoup
from aiohttp import ClientConnectorError

from src.session.aiohttp import AiohttpSession
from src.utils.user_agent import get_user_agent
from src.core.settings import load_settings
from src.utils.logger import Logger


class HabsevAPI:

    API: str = 'https://habsev.md'

    def __init__(
            self, 
            tasks_starts_at_once: int = 100, 
            proxy: Optional[str] = None,
            logger: Optional[Logger] = None
    ) -> None:
        self._session = AiohttpSession(api=self.API, proxy=proxy)
        self._semaphore = asyncio.Semaphore(tasks_starts_at_once)
        self.settings = load_settings()
        self.logger = logger or Logger()
        self._headers = {
            'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'accept-language': 'ru-RU,ru;q=0.7',
        }
        self._cookies = {
            'firstVisit': 'true',
            'i18n_redirected': 'ru',
            'language': 'ru',
            'user-theme': 'light-theme',
            'first_lang_visit': 'true',
            'getPreviousLocale': 'ru',
        }

    async def __aenter__(self) -> "HabsevAPI":
        return self
    
    async def __aexit__(self, *args) -> None:
        await self._session.close()

    async def get_categories(self) -> Dict[str, Any]:

        categories = {
            'Ð­Ð»ÐµÐºÑ‚Ñ€Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ ÐºÐ°Ð±ÐµÐ»Ð¸ Ð¸ Ð¿Ñ€Ð¾Ð²Ð¾Ð´Ð°': 'https://habsev.md/ru/cabluri-electrice-si-conductoare',
            'ÐšÐ°Ð±ÐµÐ»ÑŒÐ½Ñ‹Ðµ ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹ Ð¿Ð¾Ð·Ð¸Ñ†Ð¸Ð¾Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¸ Ð°ÐºÑÐµÑÑÑƒÐ°Ñ€Ñ‹': 'https://habsev.md/ru/accesorii-cablu',
            'ÐšÐ¾Ñ€Ð¾Ð±ÐºÐ¸ Ñ€Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ,ÑÐ»ÐµÐºÑ‚Ñ€Ð¾Ñ‰Ð¸Ñ‚ÐºÐ¸ Ð¸ Ð°ÐºÑÐµÑÑÑƒÐ°Ñ€Ñ‹': 'https://habsev.md/ru/doze-si-tablouri-electrice',
            'Ð—Ð°Ñ‰Ð¸Ñ‚Ð° ÑÐ»ÐµÐºÑ‚Ñ€Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… Ñ†ÐµÐ¿ÐµÐ¹': 'https://habsev.md/ru/protectie-si-comutare',
            'Ð Ð¾Ð·ÐµÑ‚ÐºÐ¸ Ð¸ Ð²Ñ‹ÐºÐ»ÑŽÑ‡Ð°Ñ‚ÐµÐ»Ð¸': 'https://habsev.md/ru/prize-si-intrerupatoare-1',
            'ÐšÐ¾Ð½Ñ‚Ñ€Ð¾Ð»ÑŒÐ½Ð¾-Ð¸Ð·Ð¼ÐµÑ€Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ðµ Ð¾Ð±Ð¾Ñ€ÑƒÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ': 'https://habsev.md/ru/scule-si-aparate-de-masura',
            'ÐžÑÐ²ÐµÑ‰ÐµÐ½Ð¸Ðµ': 'https://habsev.md/ru/iluminat',
            'Ð­Ð»ÐµÐºÑ‚Ñ€Ð¸Ñ‡ÐµÑÐºÐ¾Ðµ Ð¾Ñ‚Ð¾Ð¿Ð»ÐµÐ½Ð¸Ðµ': 'https://habsev.md/ru/incalzire-electrica',
            'Ð¡Ñ€ÐµÐ´ÑÑ‚Ð²Ð° Ð¸Ð½Ð´Ð¸Ð²Ð¸Ð´ÑƒÐ°Ð»ÑŒÐ½Ð¾Ð¹ Ð·Ð°Ñ‰Ð¸Ñ‚Ñ‹': 'https://habsev.md/ru/echipament-de-protectie',
            'Ð—Ð°Ð·ÐµÐ¼Ð»ÐµÐ½Ð¸Ðµ / Ð“Ñ€Ð¾Ð¼Ð¾Ð¾Ñ‚Ð²Ð¾Ð´': 'https://habsev.md/ru/impamantare-paratrasnet',
            'Ð—Ð°Ñ€ÑÐ´Ð½Ñ‹Ðµ ÑÑ‚Ð°Ð½Ñ†Ð¸Ð¸': 'https://habsev.md/ru/statie-de-incarcare',
            'ÐžÐ±Ð¾Ñ€ÑƒÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð´Ð»Ñ Ð±ÐµÑÐ¿ÐµÑ€ÐµÐ±Ð¾Ð¹Ð½Ð¾Ð³Ð¾ Ð¿Ð¸Ñ‚Ð°Ð½Ð¸Ñ': 'https://habsev.md/ru/echipament-electric-de-protectie',
            'Ð£Ð»Ð¸Ñ‡Ð½Ð¾Ðµ Ð¾ÑÐ²ÐµÑ‰ÐµÐ½Ð¸Ðµ': 'https://habsev.md/ru/iluminat-stradal-lea',
            'ÐšÐ¾Ð½Ñ†ÐµÐ²Ñ‹Ðµ Ð¸ ÑÐ¾ÐµÐ´Ð¸Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð¼ÑƒÑ„Ñ‚Ñ‹ Ð´Ð¾ 1 ÐºÐ’': 'https://habsev.md/ru/recloser',
            'ÐžÐ±Ð¾Ñ€ÑƒÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ 6-35 ÐºÐ’': 'https://habsev.md/ru/posturi-si-transformatoare',
            'Ð¤Ð¾Ñ‚Ð¾ÑÐ»ÐµÐºÑ‚Ñ€Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹': 'https://habsev.md/ru/sisteme-fotovoltaice',
        }
    
        return categories
    
    async def get_all_urls_in_category(self, url: str):
        
        self._headers['user-agent'] = get_user_agent()

        try:
            response = await self._session(
                'GET', 
                url, 
                headers=self._headers, 
                cookies=self._cookies,
            )
        except ClientConnectorError as ex:
            await asyncio.sleep(5)
            response = await self._session(
                'GET', 
                url, 
                headers=self._headers,
                cookies=self._cookies, 
            )
        

        if not response:
            return None
        
        categories_data = {}
        soup = BeautifulSoup(response, "lxml")

        subcatalog = soup.find_all('a', attrs={'class': 'subcatalog__item'})
        if not subcatalog:
            # print('Ñ‚ÑƒÑ‚ ÑƒÐ¶Ðµ ÐºÐ°Ñ‚Ð°Ð»Ð¾Ð³ Ñ‚Ð¾Ð²Ð°Ñ€Ð¾Ð²')
            pass
        else:
            for x in subcatalog:
                name = x.find('div', attrs={'class': 'title'}).text.strip()
                url = x.get('href')
                categories_data[name] = url
                self.logger.info(f'ðŸ“ŒÐ”Ð¾Ð±Ð°Ð²Ð¸Ð» ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸ÑŽ: {name}')
                # print(name, url)

            for n,u in list(categories_data.items()):
                result = await self.get_all_urls_in_category(u)
                if result:  # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼, Ñ‡Ñ‚Ð¾ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ Ð½Ðµ Ð¿ÑƒÑÑ‚Ð¾Ð¹
                    # print(f'Del {n}')
                    categories_data.pop(n)
                    categories_data.update(result) 


        return categories_data

    
    
    async def get_all_products(self, url: str) -> List[str]:

        async def _make_request(url: str, page: int = None) -> str:
            """Ð£Ð½Ð¸Ð²ÐµÑ€ÑÐ°Ð»ÑŒÐ½Ñ‹Ð¹ Ð¼ÐµÑ‚Ð¾Ð´ Ð´Ð»Ñ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°."""
            # full_url = f"https://admin.ecom.md/general/v2/category/{url}/" if page else url
            full_url = f"{url}?page={page}" if page else url
            self._headers['user-agent'] = get_user_agent()  
            # if page:
            #     headers = {
            #         'accept': 'application/json, text/plain, */*',
            #         'accept-language': 'ru',
            #         'origin': 'https://habsev.md',
            #         'priority': 'u=1, i',
            #         'token': 'gmr0nwt*yur3tnu_CKG',
            #     }
            #     params = {
            #         'page': '1',
            #         'sortBy': 'sort_priority-',
            #     }
            response = await self._session(
                    'GET', 
                    f'{full_url}', 
                    headers=self._headers, 
                    cookies=self._cookies,
                )
            
            return response

        async def check_page_num(url: str) -> int:

            html = await _make_request(url)
            soup = BeautifulSoup(html, "lxml")

            pagination_items = soup.find_all('li', class_='pagination__item')
            next_button = soup.find('span', class_='pagination__button-text', text='Ð¡Ð»ÐµÐ´ÑƒÑŽÑ‰Ð°Ñ')

            if next_button:
                next_item = next_button.find_parent('li', class_='pagination__item')
                next_index = pagination_items.index(next_item)
                previous_item = pagination_items[next_index - 1]
                page_number = int(previous_item.get_text(strip=True))
                # self.logger.info(f'ÐžÐ±Ñ‰ÐµÐµ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†: {page_number}')
                return page_number
            else:
                # self.logger.info(f'ÐžÐ±Ñ‰ÐµÐµ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†: 1')
                return 1
        
        async def fetch_page(url: str, page: int = 1) -> List[str]:

            html = await _make_request(url, page)
            soup = BeautifulSoup(html, "lxml")
            product_links = []
            products = soup.find_all('div', attrs={'class': 'product__item'})
            
            for div in products:
                url = div.find('a').get('href')
                product_links.append(f'{self.API}{url}')

            return product_links

        number_pages = await check_page_num(url)
        data = []

        for page in range(1, number_pages + 1):
            products = await fetch_page(url, page)
            data.extend(products)
        
        return data


    
    async def get_html_product(self, url: str):

        self._headers['user-agent'] = get_user_agent()

        response = await self._session(
            'GET', 
            f'{url}', 
            headers=self._headers, 
            cookies=self._cookies,
        )

        return response



    